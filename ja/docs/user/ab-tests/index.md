# A/Bテスト

複数のデプロイメントスロット間でトラフィックを分配し、推薦モデルの効果を比較します。

![A/Bテスト一覧](./ab-tests.png)

## A/Bテストの仕組み

プロジェクトレベルの推薦 API（`POST /inference/predict/project/{project_id}`）を呼び出すと、[デプロイメントスロット](../deployment-slots/)の Weight に基づいてリクエストが振り分けられます。

各スロットへのリクエストとコンバージョンイベントを記録することで、モデル間の効果を統計的に比較できます。

## A/Bテストの作成

**"New A/B Test"** ボタンから作成します:

![A/Bテスト作成](./ab-test-create.png)

設定項目:
- **名前** — テストの識別名
- **Control Slot** — ベースラインとなるデプロイメントスロット
- **Variant Slot** — 比較対象のデプロイメントスロット
- **Target Metric** — コンバージョンイベントとして記録するメトリクス名（例: `click_through_rate`）
- **Min Sample Size** — 統計的判定を行う最小サンプル数（デフォルト: 1000）
- **Confidence Level** — 有意水準（デフォルト: 0.95 = 95%信頼区間）

## コンバージョンイベントの記録

推薦されたアイテムに対してユーザーがアクション（クリック、購入など）を行った場合、以下の API でコンバージョンイベントを記録します:

```http
POST /api/v1/conversion_event/
```

## 結果の分析

A/B テスト詳細画面では、スロットごとのコンバージョン率や統計的有意性を確認できます。
